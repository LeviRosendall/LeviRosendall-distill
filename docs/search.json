{
  "articles": [
    {
      "path": "about.html",
      "title": "Learning Objectives",
      "description": "How this project meets the learning objectives",
      "author": [],
      "contents": "\nImport, manage, and clean data\nThis project imports data from several sources. The first comes from the .csv file that was downloaded from Etsy. The code for that is\n\n\nFinalEtsy <- read_csv('~/STA 518/ETSY2021/FinalEtsy.csv).\n\nThe second way that data is inputted in this project is using the {tidycensus} package.\n\n\nlibrary(tidycensus)\nreadRenviron(\"~/.Renviron\")\nstateIncome <- get_acs(geography = \"state\", \n              variables = c(medincome = \"B19013_001\"), \n              year = 2018)\n\nstatePop <- get_acs(geography = \"state\", \n              variables = c(population = \"B01003_001\"), \n              year = 2018)\n\nThe original Etsy dataset came with each observation being one sale, which made mapping it a challenge. A large amount of data manipulation went into creating a cumulative sale map. I started by creating an “empty” data frame, including a row for each day and state.\n\n\nstatesOver=c()\nsellDay=c()\nstart <- as.Date(\"07-31-21\",format=\"%m-%d-%y\")\nend   <- as.Date(\"10-08-21\",format=\"%m-%d-%y\")\n\ntheDate <- start\ntheDate\nfor(state in stateList){\n  while (theDate <= end)\n  {\n    statesOver <- c(statesOver, state)\n    sellDay <- c(sellDay, as_date(theDate))\n    theDate <- theDate + 1                    \n  }\n  theDate <- start\n}\nsellDay <- as.Date(as.POSIXct(sellDay*24*60*60, origin = \"1970-01-01\", tz=\"UTC\"))\n\nemptyTable <- data.frame(sellDay, statesOver)\n\nThis allowed me to left_join my edited Etsy table called NoDup, which consisted of a sum of signs sold to each state on each given day. For example, if there were 3 orders to Oregon and 1 order to Michigan on August 15, NoDup had two rows for August 15, one for each state. This is different from the original file which would have had 4 rows for that date.\n\n\nfullTable <- left_join(emptyTable, NoDup, by=c(\"sellDay\"=\"date\", \"statesOver\"=\"state\"))\n\nThe data table fullTable then was changed with a cumulative sum, so each state sign total was filled in. A new data frame called plotTable was then created with several different way to plot the cumulative total, and logSold was decided on.\n\n\nplotTable <- fullTable %>% \n  mutate(numSold = ifelse(cumulative==0, 0,\n                    ifelse(cumulative<5, 1,\n                    ifelse(cumulative<10, 2,\n                    ifelse(cumulative<20, 3,\n                    ifelse(cumulative<50, 4,\n                    ifelse(cumulative<100, 5,\n                    ifelse(cumulative<200, 6, 7)))))))) %>% \n  mutate(logSold=ifelse(cumulative>0, log(cumulative+1), 0)) %>% \n  mutate(day = (month(sellDay)*100+day(sellDay)) )\n\nCreate graphical displays and numerical summaries of data for exploratory analysis and presentations.\nThe cumulative sales map was created two ways, gif and html products. The code for the gif is\n\n\n#creates a cumulative gif\ngifAnim <- right_join(get_urbn_map(map = \"states\", sf = TRUE),\n                          plotTable,\n                          by = c(\"state_abbv\" = \"statesOver\"))\ngifAnim <- gifAnim %>% \n  ggplot() +\n  geom_sf(aes(fill = as.numeric(cumulative))) +\n  scale_fill_gradient(low = \"navy\", high = \"lightskyblue3\")   +   \n  labs(subtitle = \"Date: {frame_time}\", fill = \"abc\") +  \n  theme_void() +\n  theme(legend.position = c(0.15, .15))\ngifAnim <- gifAnim + transition_time(sellDay)\nloop <- animate(\n    gifAnim,\n    fps = 10,           # 10 frames per second\n    nframes = 70,  # animate with 1 frame each day\n    end_pause = 10,      # hold still for last 30 frames (3 secs)\n    renderer = gifski_renderer(\"USCumul.gif\")\n)\n\nIt effectively uses the package {urbnmapr} to get the sf (simple features) outline of the US, and identify state names. To plot the map, ggplot was used to fill the sf based on the cumulative sales ampunt, using date as a frame (70 days). Using {gifski}, the animated map was exported as a gif.\nThe second way to export the map, which is more interactive, was with the {plotly} package. The code for that is\n\n\nUSMAP <- plotTable %>% \n  plot_ly(\n    type = 'choropleth',\n    locations= ~statesOver,\n    locationmode = 'USA-states' , \n    colorscale='tempo', z=~logSold,\n    zmin=0, zmax=6,\n    text=~cumulative,\n    hoverinfo='text',\n    showscale=FALSE,\n    frame=~day) %>% \n  layout(geo=list( scope = 'usa' )) \nlibrary(htmlwidgets)\nsaveWidget(USMAP, \"USMAP.html\", selfcontained = F, libdir = \"lib\")\n\nThis code creates a plot_ly object called USMAP. It is a choropleth map with the statesOver column of plotTable as the state indicator. Its scale is preset to envelope all of the possible logSold values in plotTable. The package {htmlwidgets} is used to save it as a widget that has been embedded into this website (see “map” tab).\nThe second plot that I made was more for exploratory analysis. I wanted to see if there was a correlation between the number of signs sold to a state per 1000 people and the median income in a state. I made an accompanying scatterplot with the code\n\nCumulUS %>% \n  ggplot(mapping=aes(x=medincome, y=soldPerCapita)) + \n  geom_point(color=\"blue\")+\n  geom_quantile(color=\"black\")+\n  labs(x=\"State Median Income($)\", y=\"Signs Sold Per 1000 Capita\")+\n  theme(panel.background = element_rect(fill=\"#FFFFE0\"),\n        panel.grid.major = element_line(color=\"grey\"))\n\nI used a standard ggplot(), but changed the colors of the graph to make it fit a theme with blue and yellow, like the signs that I sold.\nWrite R programs for simulations from probabbility models and randomization-based experiments.\nThere are two different probability models I would like to showcase. The first one is a simple correlation procedure done for this project. It accompanies the scatterplot above, and is performed using the code\n\n\ncor.test(CumulUS$medincome, CumulUS$soldPerCapita, method=c(\"pearson\", \"kendall\", \"spearman\"))\n\nThis tests the probability that the variables are not correlated given the sample. It was concluded that they are correlated.\nThe second is more complicated, part of a project I am working on in CIS 661. I won’t go too in-depth about the project, just give enough context. We are investigating how physical contact contracted diseases such as Hep A are transmitted on bikes. Code was written to make a sample of the difference between case loads depending on cleaning procedures of the Citi Bike system in NYC. The code below was a way to bootstrap this sample to generalize the difference to any month of the year based on June 2013 data. 1000 samples of 330 (population size) were taken and quantiles were used as a 90% confidence interval.\n\nlibrary(infer)\nset.seed(661)\nmany_diff <- replicate(1000, sample(diffTable$difference, size=nrow(diffTable), replace=TRUE), simplify=FALSE)\nmeansDiff <- map_dbl(many_diff, mean)\n\nquantile(meansDiff, probs=c(0.05, 0.95))\n\nUse source documentation and other resources to troubleshoot and extend R programs.\nI have used source documentation for a lot of work this semester, both through google and the ?function in the console. I have found and used several packages not covered in this course, such as {tidycensus}, {plotly}, and more, as detailed in previous sections. I would like to point to my completion of the RShiny Modules to exhibit my ability to identify and correct common errors in R programs.\nMy ability to use github can be seen not only in day to day use (fork, copy, etc), but also in its use in my CIS 661 class as collaboration. My group partner and I have used it to keep our r code up to date, as well as to communicate. I have also used issues extensively in this course.\nWrite clear, efficient, and well-documented R programs.\n\n\n\n",
      "last_modified": "2021-11-10T14:27:25+00:00"
    },
    {
      "path": "EtsyMap.html",
      "title": "EtsyMap",
      "author": [],
      "contents": "\n\n\n\n\n\n\n\n\nLevi\n\n\nHome\nAbout\nProject\nMap\nMap Code\nIncome Correlation\n☰\n\n\n  \n    \n      \n        \n        \n        \n      \n      \n    \n    \n      \n  Home\n\n\n  About\n\n\n  Project\n\n\n  Map\n\n\n  Map Code\n\n\n  Income Correlation\n\n      \n  \n\n\n\n\n\n\nEtsyMap\n\n\n\n\nlibrary(readxl)\nlibrary(tidyverse)\n## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n## ✓ tibble  3.1.5     ✓ dplyr   1.0.7\n## ✓ tidyr   1.1.3     ✓ stringr 1.4.0\n## ✓ readr   1.4.0     ✓ forcats 0.5.1\n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()\nlibrary(zipcodeR)\nlibrary(urbnmapr)\nlibrary(gganimate)\nlibrary(sf)\n## Linking to GEOS 3.8.0, GDAL 3.0.4, PROJ 6.3.1\nlibrary(transformr)\n## \n## Attaching package: 'transformr'\n## The following object is masked from 'package:sf':\n## \n##     st_normalize\nlibrary(lubridate)\n## \n## Attaching package: 'lubridate'\n## The following objects are masked from 'package:base':\n## \n##     date, intersect, setdiff, union\nlibrary(plotly)\n## \n## Attaching package: 'plotly'\n## The following object is masked from 'package:ggplot2':\n## \n##     last_plot\n## The following object is masked from 'package:stats':\n## \n##     filter\n## The following object is masked from 'package:graphics':\n## \n##     layout\nFinalEtsy <- read_csv('~/STA 518/ETSY2021/FinalEtsy.csv')\n## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   .default = col_character(),\n##   `Order ID` = col_double(),\n##   `Number of Items` = col_double(),\n##   `Order Value` = col_double(),\n##   `Coupon Code` = col_logical(),\n##   `Coupon Details` = col_logical(),\n##   `Discount Amount` = col_double(),\n##   `Shipping Discount` = col_double(),\n##   Shipping = col_double(),\n##   `Sales Tax` = col_double(),\n##   `Order Total` = col_double(),\n##   Status = col_logical(),\n##   `Card Processing Fees` = col_double(),\n##   `Order Net` = col_double(),\n##   `Adjusted Order Total` = col_double(),\n##   `Adjusted Card Processing Fees` = col_double(),\n##   `Adjusted Net Order Amount` = col_double(),\n##   `InPerson Discount` = col_logical(),\n##   `InPerson Location` = col_logical()\n## )\n## ℹ Use `spec()` for the full column specifications.\nState_Ind <- right_join(get_urbn_map(map = \"states\", sf = TRUE),\n                          FinalEtsy,\n                          by = c(\"state_abbv\" = \"Ship State\"))\n## old-style crs object detected; please recreate object with a recent sf::st_crs()\nState_Ind_Year <- State_Ind %>% \n  filter(`Ship Country`=='United States' & `state_abbv` != 'PR') %>% \n  mutate(actualDate=as.Date(mdy(`Sale Date`))) %>% \n  mutate(day=actualDate-mdy('07-28-2021')) %>% \n  arrange(day)\nAnimationDataTry <- FinalEtsy %>% \n  filter(`Ship Country`==\"United States\") %>% \n  mutate(date=mdy(`Sale Date`)) %>% \n  arrange(date) %>% \n  mutate(state=`Ship State`) %>% \n  group_by(state, date) %>% \n  summarise(Total_Sold=sum(`Number of Items`), date) %>% \n  arrange(date)\n## `summarise()` has grouped output by 'state', 'date'. You can override using the `.groups` argument.\nNoDup <- AnimationDataTry %>% distinct()\n\nstateListData <- State_Ind_Year %>% \n  group_by(state_abbv) %>% \n  summarise(total=sum(`Number of Items`))\n\nstateList <- stateListData$state_abbv\n\nstatesOver=c()\nsellDay=c()\nstart <- as.Date(\"07-31-21\",format=\"%m-%d-%y\")\nend   <- as.Date(\"10-08-21\",format=\"%m-%d-%y\")\n\ntheDate <- start\ntheDate\n## [1] \"2021-07-31\"\nfor(state in stateList){\n  while (theDate <= end)\n  {\n    statesOver <- c(statesOver, state)\n    sellDay <- c(sellDay, as_date(theDate))\n    theDate <- theDate + 1                    \n  }\n  theDate <- start\n}\nsellDay <- as.Date(as.POSIXct(sellDay*24*60*60, origin = \"1970-01-01\", tz=\"UTC\"))\n\nemptyTable <- data.frame(sellDay, statesOver)\n\nfullTable <- left_join(emptyTable, NoDup, by=c(\"sellDay\"=\"date\", \"statesOver\"=\"state\"))\n\nfullTable <- fullTable %>% \n  mutate(items=ifelse(is.na(Total_Sold), 0,Total_Sold)) %>% \n  group_by(statesOver) %>% \n  summarise(sellDay, statesOver, items, cumulative = cumsum(items))\n## `summarise()` has grouped output by 'statesOver'. You can override using the `.groups` argument.\nplotTable <- fullTable %>% \n  mutate(numSold = ifelse(cumulative==0, 0,\n                    ifelse(cumulative<5, 1,\n                    ifelse(cumulative<10, 2,\n                    ifelse(cumulative<20, 3,\n                    ifelse(cumulative<50, 4,\n                    ifelse(cumulative<100, 5,\n                    ifelse(cumulative<200, 6, 7)))))))) %>% \n  mutate(logSold=ifelse(cumulative>0, log(cumulative+1), 0)) %>% \n  mutate(day = (month(sellDay)*100+day(sellDay)) )\nUSMAP <- plotTable %>% \n  plot_ly(\n    type = 'choropleth',\n    locations= ~statesOver,\n    locationmode = 'USA-states' , \n    colorscale='tempo', z=~logSold,\n    zmin=0, zmax=6,\n    text=~cumulative,\n    hoverinfo='text',\n    showscale=FALSE,\n    frame=~day) %>% \n  layout(geo=list( scope = 'usa' )) \nlibrary(htmlwidgets)\nsaveWidget(USMAP, \"USMAP.html\", selfcontained = F, libdir = \"lib\")\n#This map is a gif. If you would like to view an interactive version, go to the “Map” tab.\nknitr::include_graphics(\"USCumul.gif\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n// add bootstrap table styles to pandoc tables\nfunction bootstrapStylePandocTables() {\n  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');\n}\n$(document).ready(function () {\n  bootstrapStylePandocTables();\n});\n\n\n\n$(document).ready(function () {\n  window.buildTabsets(\"TOC\");\n});\n\n$(document).ready(function () {\n  $('.tabset-dropdown > .nav-tabs > li').click(function () {\n    $(this).parent().toggleClass('nav-tabs-open');\n  });\n});\n\n  (function () {\n    var script = document.createElement(\"script\");\n    script.type = \"text/javascript\";\n    script.src  = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\";\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\n  })();\n",
      "last_modified": "2021-11-10T14:27:31+00:00"
    },
    {
      "path": "IncomeCorrelation.html",
      "title": "IncomeCorrelation",
      "author": [],
      "contents": "\n\n\n\n\n\n\n\n\nLevi\n\n\nHome\nAbout\nProject\nMap\nMap Code\nIncome Correlation\n☰\n\n\n  \n    \n      \n        \n        \n        \n      \n      \n    \n    \n      \n  Home\n\n\n  About\n\n\n  Project\n\n\n  Map\n\n\n  Map Code\n\n\n  Income Correlation\n\n      \n  \n\n\n\n\n\n\nIncomeCorrelation\n\n\n\n\n\nR Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(urbnmapr)\nCensusData <- read_excel('~/STA 518/ETSY2021/Census Data.xlsx')\nFinalEtsy <- read_csv('~/STA 518/ETSY2021/FinalEtsy.csv')\n## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   .default = col_character(),\n##   `Order ID` = col_double(),\n##   `Number of Items` = col_double(),\n##   `Order Value` = col_double(),\n##   `Coupon Code` = col_logical(),\n##   `Coupon Details` = col_logical(),\n##   `Discount Amount` = col_double(),\n##   `Shipping Discount` = col_double(),\n##   Shipping = col_double(),\n##   `Sales Tax` = col_double(),\n##   `Order Total` = col_double(),\n##   Status = col_logical(),\n##   `Card Processing Fees` = col_double(),\n##   `Order Net` = col_double(),\n##   `Adjusted Order Total` = col_double(),\n##   `Adjusted Card Processing Fees` = col_double(),\n##   `Adjusted Net Order Amount` = col_double(),\n##   `InPerson Discount` = col_logical(),\n##   `InPerson Location` = col_logical()\n## )\n## ℹ Use `spec()` for the full column specifications.\n#Creates a dataset of only US orders\nUS_ETSY2021 <- FinalEtsy %>% \n  filter(`Ship Country`==\"United States\")\n#Gets median income by state\nlibrary(tidycensus)\n#census_api_key(\"74dffda41385b000bbc7c51fcc2b32468769dce9\", install=TRUE)\nreadRenviron(\"~/.Renviron\")\nstateIncome <- get_acs(geography = \"state\", \n              variables = c(medincome = \"B19013_001\"), \n              year = 2018)\n## Getting data from the 2014-2018 5-year ACS\nstatePop <- get_acs(geography = \"state\", \n              variables = c(population = \"B01003_001\"), \n              year = 2018)\n## Getting data from the 2014-2018 5-year ACS\nv17 <- load_variables(2018, \"acs5\", cache = TRUE)\n\n#View(v17)\nCumulUS <- US_ETSY2021 %>% \n  group_by(`Ship State`) %>% \n  summarise(soldToState=sum(`Number of Items`))\n\nstateListAbbv <- get_urbn_map(map = \"states\") %>% \n  select(state_abbv, state_name) %>% \n  distinct() %>% \n  rbind(c(\"PR\", \"Puerto Rico\"))\n\nCumulUS <- left_join(CumulUS, stateListAbbv, by=c(\"Ship State\"=\"state_abbv\"))\n\nCumulUS <- left_join(CumulUS, stateIncome, by=c(\"state_name\"=\"NAME\")) %>% \n  select(soldToState, state_name, medincome=estimate)\n\nCumulUS <- left_join(CumulUS, statePop, by=c(\"state_name\"=\"NAME\")) %>% \n  select(soldToState, state_name, medincome, population=estimate) %>% \n  mutate(soldPerCapita=1000*soldToState/population)\nCumulUS %>% \n  ggplot(mapping=aes(x=medincome, y=soldPerCapita)) + \n  geom_point(color=\"blue\")+\n  geom_quantile(color=\"black\")+\n  labs(x=\"State Median Income($)\", y=\"Signs Sold Per 1000 Capita\")+\n  theme(panel.background = element_rect(fill=\"#FFFFE0\"),\n        panel.grid.major = element_line(color=\"grey\"))\n## Smoothing formula not specified. Using: y ~ x\n\ncor.test(CumulUS$medincome, CumulUS$soldPerCapita, method=c(\"pearson\", \"kendall\", \"spearman\"))\n## \n##  Pearson's product-moment correlation\n## \n## data:  CumulUS$medincome and CumulUS$soldPerCapita\n## t = 4.4474, df = 50, p-value = 4.854e-05\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.3036150 0.7031413\n## sample estimates:\n##       cor \n## 0.5324023\n\n\n\n\n\n\n\n\n\n\n\n\n\n// add bootstrap table styles to pandoc tables\nfunction bootstrapStylePandocTables() {\n  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');\n}\n$(document).ready(function () {\n  bootstrapStylePandocTables();\n});\n\n\n\n$(document).ready(function () {\n  window.buildTabsets(\"TOC\");\n});\n\n$(document).ready(function () {\n  $('.tabset-dropdown > .nav-tabs > li').click(function () {\n    $(this).parent().toggleClass('nav-tabs-open');\n  });\n});\n\n  (function () {\n    var script = document.createElement(\"script\");\n    script.type = \"text/javascript\";\n    script.src  = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\";\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\n  })();\n",
      "last_modified": "2021-11-10T14:27:34+00:00"
    },
    {
      "path": "index.html",
      "title": "Levi Rosendall STA 518 Project",
      "description": "Welcome to the website. I hope you enjoy it!\n",
      "author": [],
      "contents": "\nProject Overview:\nFrom July 31 to October 8, I ran an ETSY shop selling BELIEVE signs from the Apple TV+ show ‘Ted Lasso’. For my project I would like to not only map where I sent signs, but also do some analysis on the locations within the United States where I shipped. Something I am thinking about is how many signs were sent to (zipcode, county, state) vs the income of that place. The income data will come from US Census Data.\nPlan of Action:\nCreate an animated map of where I have shipped\nCreate tables/correlations between ship area vs income\nCompile the above into a website that makes viewing my project results easy\n\n\n\n",
      "last_modified": "2021-11-10T14:27:35+00:00"
    },
    {
      "path": "project.html",
      "title": "Project",
      "author": [],
      "contents": "\nProject Overview:\nFrom July 31 to October 8, I ran an ETSY shop selling BELIEVE signs from the Apple TV+ show ‘Ted Lasso’. For my project I would like to not only map where I sent signs, but also do some analysis on the locations within the United States where I shipped. Something I am thinking about is how many signs were sent to (zipcode, county, state) vs the income of that place. The income data will come from US Census Data.\nPlan of Action:\nCreate an animated map of where I have shipped\nCreate tables/correlations between ship area vs income\nCompile the above into a website that makes viewing my project results easy\n\n\n\n",
      "last_modified": "2021-11-10T14:27:35+00:00"
    },
    {
      "path": "Proposal.html",
      "author": [],
      "contents": "\nProject Overview: From July 31 to October 8, I ran an ETSY shop selling BELIEVE signs from the Apple TV+ show ‘Ted Lasso’. For my project I would like to not only map where I sent signs, but also do some analysis on the locations within the United States where I shipped. Something I am thinking about is how many signs were sent to (zipcode, county, state) vs the income of that place. The income data will come from US Census Data.\nPlan of Action: Create an animated map of where I have shipped Create tables/correlations between ship area vs income Compile the above into a website that makes viewing my project results easy\nComments:\n\n\n",
      "last_modified": "2021-11-10T14:27:36+00:00"
    }
  ],
  "collections": []
}
