{
  "articles": [
    {
      "path": "about.html",
      "title": "Learning Objectives",
      "description": "How this project meets the learning objectives",
      "author": [],
      "contents": "\nGithub link\nhttps://github.com/LeviRosendall/LeviRosendall-distill\nDue to the dataset having identifiable information, it is not included in the github repo.\nImport, manage, and clean data\nThis project imports data from several sources. The first comes from the .csv file that was downloaded from Etsy. The code for that is\n\n\nFinalEtsy <- read_csv('~/STA 518/ETSY2021/FinalEtsy.csv).\n\nThe second way that data is inputted in this project is using the {tidycensus} package.\n\n\nlibrary(tidycensus)\nreadRenviron(\"~/.Renviron\")\nstateIncome <- get_acs(geography = \"state\", \n              variables = c(medincome = \"B19013_001\"), \n              year = 2018)\n\nstatePop <- get_acs(geography = \"state\", \n              variables = c(population = \"B01003_001\"), \n              year = 2018)\n\nThe original Etsy dataset came with each observation being one sale, which made mapping it a challenge. A large amount of data manipulation went into creating a cumulative sale map. I started by creating an “empty” data frame, including a row for each day and state.\n\n\nstatesOver=c()\nsellDay=c()\nstart <- as.Date(\"07-31-21\",format=\"%m-%d-%y\")\nend   <- as.Date(\"10-08-21\",format=\"%m-%d-%y\")\n\ntheDate <- start\ntheDate\nfor(state in stateList){\n  while (theDate <= end)\n  {\n    statesOver <- c(statesOver, state)\n    sellDay <- c(sellDay, as_date(theDate))\n    theDate <- theDate + 1                    \n  }\n  theDate <- start\n}\nsellDay <- as.Date(as.POSIXct(sellDay*24*60*60, origin = \"1970-01-01\", tz=\"UTC\"))\n\nemptyTable <- data.frame(sellDay, statesOver)\n\nThis allowed me to left_join my edited Etsy table called NoDup, which consisted of a sum of signs sold to each state on each given day. For example, if there were 3 orders to Oregon and 1 order to Michigan on August 15, NoDup had two rows for August 15, one for each state. This is different from the original file which would have had 4 rows for that date.\n\n\nfullTable <- left_join(emptyTable, NoDup, by=c(\"sellDay\"=\"date\", \"statesOver\"=\"state\"))\n\nThe data table fullTable then was changed with a cumulative sum, so each state sign total was filled in. A new data frame called plotTable was then created with several different way to plot the cumulative total, and logSold was decided on.\n\n\nplotTable <- fullTable %>% \n  mutate(numSold = ifelse(cumulative==0, 0,\n                    ifelse(cumulative<5, 1,\n                    ifelse(cumulative<10, 2,\n                    ifelse(cumulative<20, 3,\n                    ifelse(cumulative<50, 4,\n                    ifelse(cumulative<100, 5,\n                    ifelse(cumulative<200, 6, 7)))))))) %>% \n  mutate(logSold=ifelse(cumulative>0, log(cumulative+1), 0)) %>% \n  mutate(day = (month(sellDay)*100+day(sellDay)) )\n\nCreate graphical displays and numerical summaries of data for exploratory analysis and presentations.\nThe cumulative sales map was created two ways, gif and html products. The code for the gif is\n\n\n#creates a cumulative gif\ngifAnim <- right_join(get_urbn_map(map = \"states\", sf = TRUE),\n                          plotTable,\n                          by = c(\"state_abbv\" = \"statesOver\"))\ngifAnim <- gifAnim %>% \n  ggplot() +\n  geom_sf(aes(fill = as.numeric(cumulative))) +\n  scale_fill_gradient(low = \"navy\", high = \"lightskyblue3\")   +   \n  labs(subtitle = \"Date: {frame_time}\", fill = \"abc\") +  \n  theme_void() +\n  theme(legend.position = c(0.15, .15))\ngifAnim <- gifAnim + transition_time(sellDay)\nloop <- animate(\n    gifAnim,\n    fps = 10,           # 10 frames per second\n    nframes = 70,  # animate with 1 frame each day\n    end_pause = 10,      # hold still for last 30 frames (3 secs)\n    renderer = gifski_renderer(\"USCumul.gif\")\n)\n\nIt effectively uses the package {urbnmapr} to get the sf (simple features) outline of the US, and identify state names. To plot the map, ggplot was used to fill the sf based on the cumulative sales ampunt, using date as a frame (70 days). Using {gifski}, the animated map was exported as a gif.\nThe second way to export the map, which is more interactive, was with the {plotly} package. The code for that is\n\n\nUSMAP <- plotTable %>% \n  plot_ly(\n    type = 'choropleth',\n    locations= ~statesOver,\n    locationmode = 'USA-states' , \n    colorscale='tempo', z=~logSold,\n    zmin=0, zmax=6,\n    text=~cumulative,\n    hoverinfo='text',\n    showscale=FALSE,\n    frame=~day) %>% \n  layout(geo=list( scope = 'usa' )) \nlibrary(htmlwidgets)\nsaveWidget(USMAP, \"USMAP.html\", selfcontained = F, libdir = \"lib\")\n\nThis code creates a plot_ly object called USMAP. It is a choropleth map with the statesOver column of plotTable as the state indicator. Its scale is preset to envelope all of the possible logSold values in plotTable. The package {htmlwidgets} is used to save it as a widget that has been embedded into this website (see “map” tab).\nThe second plot that I made was more for exploratory analysis. I wanted to see if there was a correlation between the number of signs sold to a state per 1000 people and the median income in a state. I made an accompanying scatterplot with the code\n\nCumulUS %>% \n  ggplot(mapping=aes(x=medincome, y=soldPerCapita)) + \n  geom_point(color=\"blue\")+\n  geom_quantile(color=\"black\")+\n  labs(x=\"State Median Income($)\", y=\"Signs Sold Per 1000 Capita\")+\n  theme(panel.background = element_rect(fill=\"#FFFFE0\"),\n        panel.grid.major = element_line(color=\"grey\"))\n\nI used a standard ggplot(), but changed the colors of the graph to make it fit a theme with blue and yellow, like the signs that I sold.\nWrite R programs for simulations from probabbility models and randomization-based experiments.\nThere are two different probability models I would like to showcase. The first one is a simple correlation procedure done for this project. It accompanies the scatterplot above, and is performed using the code\n\n\ncor.test(CumulUS$medincome, CumulUS$soldPerCapita, method=c(\"pearson\", \"kendall\", \"spearman\"))\n\nThis tests the probability that the variables are not correlated given the sample. It was concluded that they are correlated.\nThe second is more complicated, part of a project I am working on in CIS 661. I won’t go too in-depth about the project, just give enough context. We are investigating how physical contact contracted diseases such as Hep A are transmitted on bikes. Code was written to make a sample of the difference between case loads depending on cleaning procedures of the Citi Bike system in NYC. The code below was a way to bootstrap this sample to generalize the difference to any month of the year based on June 2013 data. 1000 samples of 330 (population size) were taken and quantiles were used as a 90% confidence interval.\n\nlibrary(infer)\nset.seed(661)\nmany_diff <- replicate(1000, sample(diffTable$difference, size=nrow(diffTable), replace=TRUE), simplify=FALSE)\nmeansDiff <- map_dbl(many_diff, mean)\n\nquantile(meansDiff, probs=c(0.05, 0.95))\n\nUse source documentation and other resources to troubleshoot and extend R programs.\nI have used source documentation for a lot of work this semester, both through google and the ?function in the console. I have found and used several packages not covered in this course, such as {tidycensus}, {plotly}, and more, as detailed in previous sections. I would like to point to my completion of the RShiny Modules to exhibit my ability to identify and correct common errors in R programs.\nMy ability to use github can be seen not only in day to day use (fork, copy, etc), but also in its use in my CIS 661 class as collaboration. My group partner and I have used it to keep our r code up to date, as well as to communicate. I have also used issues extensively in this course.\nWrite clear, efficient, and well-documented R programs.\nI have used github as my workflow, saving all of my code to the cloud so that it is easily accessible to me and others. This allows for seamless workflow for me, and also for others to suggest changes to my code.\nI think that this page documents my ability to write professional reports in R Markdown. This entire website was made in R, being exported in html to this website using the {distill} package.\nFinally, I have written clear comments throughout my code to help me go back through at a later date, as well as to allow others to view my code and know what is going on. Below is an example of a comment.\n\n#Gets median income by state  #decided to use this API because the uploaded dataset was for zip codes  #originally planned on doing zipcodes, but there wasnt enough data for that to be feasible\n\nThis comment not only describes what happens in that code block, but also explains what was tried first to get to this point. I had originally tried to do a more focused look at sales by zipcode, but a lack in data required me to do a more broad pictured sales by state analysis.\nWhat this semester taught me.\nThroughout this semester, I have gotten a good overall idea of what coding with others is about. I was able to take part in its collaborative nature, both through interaction with classmates and searching through github for coding help. While I was working individually on my project, I was able to work with others to help them with their code and get advice on my own. For projects in other classes, I used R with my classmates to create meaningful data analysis, as shown above with my CIS 661 project.\nI feel I deserve an A in this class, not as a mark that I followed directions, but rather that I created my own path to discover and work towards mastering the above learning objectives. I was able to utilize the resources put forth by Bradford, classmates, and the github community to create several projects based in R, as well as cultivate my understanding of the language and its capabilities. I hope to use R in future coursework and work experiences.\nOne thing that I would potentially change about the course is the length of the preparations. I felt that the RStudio Primers were the most helpful for me, while the textbook readings were less helpful. Limiting textbook reading to the most important sections and leaning on the RStudio Primer to solidify understanding could make the preparations more efficient.\n\n\n\n",
      "last_modified": "2021-12-02T21:11:41+00:00"
    },
    {
      "path": "EtsyMap.html",
      "title": "Etsy Map Code",
      "author": [],
      "contents": "\n\n\n#installing all of the packages needed for this code to run. Much of it was added as needed.\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(zipcodeR)\nlibrary(urbnmapr)\nlibrary(gganimate)\nlibrary(sf)\nlibrary(transformr)\nlibrary(lubridate)\nlibrary(plotly)\nFinalEtsy <- read_csv('~/STA 518/ETSY2021/FinalEtsy.csv')\n\n\n\n\n\n#Using urbnmapr to get the geometry of states added to this dataset, matched using state_abbv\nState_Ind <- right_join(get_urbn_map(map = \"states\", sf = TRUE),\n                          FinalEtsy,\n                          by = c(\"state_abbv\" = \"Ship State\"))\n\n#filtering all non-US orders and Puerto Rico orders out of the dataset\n#changing date to be a date, not char\n#creating a numeric variable called day that is days since start that the order was placed, and arranging the data by day\nState_Ind_Year <- State_Ind %>% \n  filter(`Ship Country`=='United States' & `state_abbv` != 'PR') %>% \n  mutate(actualDate=as.Date(mdy(`Sale Date`))) %>% \n  mutate(day=actualDate-mdy('07-28-2021')) %>% \n  arrange(day)\n\n\n\n\n\n#Filters original data by US only\n#creates a new data column and arranges by it\n#duplicates state column, groups by state and date\n#summarises to get sum of signs sold in day, keeps date and arranges by it.\nAnimationDataTry <- FinalEtsy %>% \n  filter(`Ship Country`==\"United States\") %>% \n  mutate(date=mdy(`Sale Date`)) %>% \n  arrange(date) %>% \n  mutate(state=`Ship State`) %>% \n  group_by(state, date) %>% \n  summarise(Total_Sold=sum(`Number of Items`), date) %>% \n  arrange(date)\n\n#removes any duplicate rows\nNoDup <- AnimationDataTry %>% distinct()\n\n#Uses summarise and group_by to get a column with all state names\nstateListData <- State_Ind_Year %>% \n  group_by(state_abbv) %>% \n  summarise(total=sum(`Number of Items`))\n\n#stores just the state names into a dataframe\n#did this as opposed to using an api because it was readily available\nstateList <- stateListData$state_abbv\n\n#creates two empty vectors for states and day of sale\nstatesOver=c()\nsellDay=c()\n#formats start and end dates for while loop below\nstart <- as.Date(\"07-31-21\",format=\"%m-%d-%y\")\nend   <- as.Date(\"10-08-21\",format=\"%m-%d-%y\")\n\n#stores start in variable called theDate and checks to make sure it ran properly\ntheDate <- start\ntheDate\n\n\n[1] \"2021-07-31\"\n\n#this for loop interates through each state and possible sell day, creating two populated vectors\nfor(state in stateList){\n  while (theDate <= end)\n  {\n    statesOver <- c(statesOver, state)\n    sellDay <- c(sellDay, as_date(theDate))\n    theDate <- theDate + 1                    \n  }\n  theDate <- start\n}\n#changes sellDay into a date again\nsellDay <- as.Date(as.POSIXct(sellDay*24*60*60, origin = \"1970-01-01\", tz=\"UTC\"))\n\n#joins the two populated vectors to start a dataset\nemptyTable <- data.frame(sellDay, statesOver)\n\n#left_joins emptyTable and the population data from NoDup\nfullTable <- left_join(emptyTable, NoDup, by=c(\"sellDay\"=\"date\", \"statesOver\"=\"state\"))\n\n#makes a new items variable, replacing NA in Total_Sold with 0\n#group_by statesOver and adds a cumulative sum by state\nfullTable <- fullTable %>% \n  mutate(items=ifelse(is.na(Total_Sold), 0,Total_Sold)) %>% \n  group_by(statesOver) %>% \n  summarise(sellDay, statesOver, items, cumulative = cumsum(items))\n\n#creates several ways to shade a choropleth map\n#This was done because shading with the raw value makes states with less than 50 signs barely change color due to California (over 200 signs shipped there) \n#makes a new day variable that will be iterable to plotly\nplotTable <- fullTable %>% \n  mutate(numSold = ifelse(cumulative==0, 0,\n                    ifelse(cumulative<5, 1,\n                    ifelse(cumulative<10, 2,\n                    ifelse(cumulative<20, 3,\n                    ifelse(cumulative<50, 4,\n                    ifelse(cumulative<100, 5,\n                    ifelse(cumulative<200, 6, 7)))))))) %>% \n  mutate(logSold=ifelse(cumulative>0, log(cumulative+1), 0)) %>% \n  mutate(day = (month(sellDay)*100+day(sellDay)) )\n\n#only needed once\n#write_csv(plotTable, here::here(\"plotTable.csv\"))\n\n\n\n\n\n#creates a choropleth using states as outlines, shading with logsold for best shading, a scale that is just above the highest logsold value to make cumulative shading look better, hovering gives the number of signs shipped to that state, frame is the day variable created above\nUSMAP <- plotTable %>% \n  plot_ly(\n    type = 'choropleth',\n    locations= ~statesOver,\n    locationmode = 'USA-states' , \n    colorscale='tempo', z=~logSold,\n    zmin=0, zmax=6,\n    text=~cumulative,\n    hoverinfo='text',\n    showscale=FALSE,\n    frame=~as.Date(sellDay)) %>% \n  layout(geo=list( scope = 'usa' )) \n\n#saves the above \nlibrary(htmlwidgets)\nsaveWidget(USMAP, \"USMAP.html\", selfcontained = F, libdir = \"lib\")\n\n\n\n#This map is a gif. If you would like to view an interactive version, go to the “Map” tab.\n\n\nknitr::include_graphics(\"USCumul.gif\")\n\n\n\n\n\n\nhtmltools::tags$iframe(\n  width = \"1024\",\n  height = \"768\",\n  src = \"USMAP.html\", \n  scrolling = \"no\", \n  seamless = \"seamless\",\n  frameBorder = \"0\"#,\n  #style=\"-webkit-transform:scale(0.5);-moz-transform-scale(0.5);\"\n)\n\n\n\n\n\n\n\n",
      "last_modified": "2021-12-02T21:11:48+00:00"
    },
    {
      "path": "IncomeCorrelation.html",
      "title": "Income Correlation Conclusions",
      "author": [],
      "contents": "\nOverview\nAn overarching theme for our classes project focused on diversity. For that reason, I wanted to try to see if there was any bias present in my online store, specifically income bias. To do that, I used {tidycensus} to gain median income and population for each state. Using state population, I created a variable for signs sold per capita in each state. Plotting this against the median income of a state with lower and upper quartiles suggested that the higher the income of a state, the more signs were sold.\nThis led me to perform a correlation test. I chose this over regression because I was simply interested in the relationship, not predicting how many signs would be sold over time. This correlation test confirmed my suspicions, revealing that there was a moderately strong positive correlation between the two variables with a p-value smaller than 0.0001.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  CumulUS$medincome and CumulUS$soldPerCapita\nt = 4.4474, df = 50, p-value = 4.854e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3036150 0.7031413\nsample estimates:\n      cor \n0.5324023 \n\n\n\n\n",
      "last_modified": "2021-12-02T21:11:52+00:00"
    },
    {
      "path": "IncomeCorrelationCode.html",
      "title": "Income Correlation Code",
      "author": [],
      "contents": "\n\n\n#loads necessary libraries\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(urbnmapr)\n\n#loads two datasets, one from Etsy, one from online census data\nCensusData <- read_excel('~/STA 518/ETSY2021/Census Data.xlsx')\nFinalEtsy <- read_csv('~/STA 518/ETSY2021/FinalEtsy.csv')\n\n\n\n\n\n#Creates a dataset of only US orders\nUS_ETSY2021 <- FinalEtsy %>% \n  filter(`Ship Country`==\"United States\")\n\n\n\n\n\n#Gets median income by state\n#decided to use this API because the uploaded dataset was for zip codes\n#originally planned on doing zipcodes, but there wasnt enough data for that to be feasible\nlibrary(tidycensus)\n#census_api_key(\"74dffda41385b000bbc7c51fcc2b32468769dce9\", install=TRUE)\nreadRenviron(\"~/.Renviron\")\nstateIncome <- get_acs(geography = \"state\", \n              variables = c(medincome = \"B19013_001\"), \n              year = 2018)\n\nstatePop <- get_acs(geography = \"state\", \n              variables = c(population = \"B01003_001\"), \n              year = 2018)\n\nv17 <- load_variables(2018, \"acs5\", cache = TRUE)\n\n#View(v17)\n\n\n\n\n\n#gets a sum of items sold for each state\nCumulUS <- US_ETSY2021 %>% \n  group_by(`Ship State`) %>% \n  summarise(soldToState=sum(`Number of Items`))\n\n#uses the urbnmapr package to get a list of states and abbreviations, manually added Puerto Rico\nstateListAbbv <- get_urbn_map(map = \"states\") %>% \n  select(state_abbv, state_name) %>% \n  distinct() %>% \n  rbind(c(\"PR\", \"Puerto Rico\"))\n\n#Joined my two datasets to get state names in from urbnmapr\nCumulUS <- left_join(CumulUS, stateListAbbv, by=c(\"Ship State\"=\"state_abbv\"))\n\n#joined that dataset with the stateIncome dataset from the census api, by the state_name\nCumulUS <- left_join(CumulUS, stateIncome, by=c(\"state_name\"=\"NAME\")) %>% \n  select(soldToState, state_name, medincome=estimate)\n\n#joined dataset with the statePop dataset to make a new variable of soldPerCapita, standardizing the number of items sold in each state\nCumulUS <- left_join(CumulUS, statePop, by=c(\"state_name\"=\"NAME\")) %>% \n  select(soldToState, state_name, medincome, population=estimate) %>% \n  mutate(soldPerCapita=1000*soldToState/population)\n\n\n\n\n\n#creates a scatterplot with quantiles for visualizing the correlation\nCumulUS %>% \n  ggplot(mapping=aes(x=medincome, y=soldPerCapita)) + \n  geom_point(color=\"blue\")+\n  geom_quantile(color=\"black\", quantiles = c(0.25, 0.75))+\n  labs(x=\"State Median Income($)\", y=\"Signs Sold Per 1000 Capita\")+\n  theme(panel.background = element_rect(fill=\"#FFFFE0\"),\n        panel.grid.major = element_line(color=\"grey\"))\n\n\n\n#performs correlation test\ncor.test(CumulUS$medincome, CumulUS$soldPerCapita, method=c(\"pearson\", \"kendall\", \"spearman\"))\n\n\n\n    Pearson's product-moment correlation\n\ndata:  CumulUS$medincome and CumulUS$soldPerCapita\nt = 4.4474, df = 50, p-value = 4.854e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3036150 0.7031413\nsample estimates:\n      cor \n0.5324023 \n\n\n\n\n",
      "last_modified": "2021-12-02T21:11:54+00:00"
    },
    {
      "path": "index.html",
      "title": "Levi Rosendall STA 518 Project",
      "description": "A website outlining my STA 518 Project and how it meets the objectives for this class.\n",
      "author": [],
      "contents": "\nProject Overview:\nFrom July 31 to October 8, I ran an ETSY shop selling BELIEVE signs from the Apple TV+ show ‘Ted Lasso’. For my project I would like to not only map where I sent signs, but also do some analysis on the locations within the United States where I shipped. Something I am thinking about is how many signs were sent to (zipcode, county, state) vs the income of that place. The income data will come from US Census Data.\nPlan of Action:\nCreate an animated map of where I have shipped\nCreate tables/correlations between ship area vs income\nCompile the above into a website that makes viewing my project results easy\n\n\n\n",
      "last_modified": "2021-12-02T21:11:55+00:00"
    },
    {
      "path": "MapEmbed.html",
      "title": "Etsy Interactive Sales Map",
      "author": [],
      "contents": "\nActivities\nFind what day/week/month had the highest sales volume\nSee how many signs were shipped to your state\nSee how many signs were sold each Friday (The day that Ted Lasso Episodes are released.)\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2021-12-02T21:11:55+00:00"
    },
    {
      "path": "project.html",
      "title": "Project",
      "author": [],
      "contents": "\nProject Overview:\nFrom July 31 to October 8, I ran an ETSY shop selling BELIEVE signs from the Apple TV+ show ‘Ted Lasso’. For my project I would like to not only map where I sent signs, but also do some analysis on the locations within the United States where I shipped. Something I am thinking about is how many signs were sent to (zipcode, county, state) vs the income of that place. The income data will come from US Census Data.\nPlan of Action:\nCreate an animated map of where I have shipped\nCreate tables/correlations between ship area vs income\nCompile the above into a website that makes viewing my project results easy\n\n\n\n",
      "last_modified": "2021-12-02T21:11:56+00:00"
    },
    {
      "path": "Proposal.html",
      "author": [],
      "contents": "\nProject Overview: From July 31 to October 8, I ran an ETSY shop selling BELIEVE signs from the Apple TV+ show ‘Ted Lasso’. For my project I would like to not only map where I sent signs, but also do some analysis on the locations within the United States where I shipped. Something I am thinking about is how many signs were sent to (zipcode, county, state) vs the income of that place. The income data will come from US Census Data.\nPlan of Action: Create an animated map of where I have shipped Create tables/correlations between ship area vs income Compile the above into a website that makes viewing my project results easy\nComments:\n\n\n",
      "last_modified": "2021-12-02T21:11:56+00:00"
    }
  ],
  "collections": []
}
